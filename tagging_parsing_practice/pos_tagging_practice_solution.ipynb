{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "pos_tagging_practice_solution.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBD-EN-2023-A-Electives/blob/main/tagging_parsing_practice/pos_tagging_practice_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvwbQnTvBRh"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repository_name = \"NLP-MBD-EN-2023-A-Electives\"\n",
        "repository_url = 'https://github.com/acastellanos-ie/' + repository_name"
      ],
      "metadata": {
        "id": "6tH3dTcuWWM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7mC64KvlwP"
      },
      "source": [
        "! git clone $repository_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecfec2Y4v6e9"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIep7l0jvtUB"
      },
      "source": [
        "! pip install -Uqqr $repository_name/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDzMQwpyODo"
      },
      "source": [
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVSW0c_dcefp"
      },
      "source": [
        "# Parts of Speech Tagging Practice\n",
        "\n",
        "The purpose of this practical session is to experiment with Part-of-Speech tagging, using the tools provided by NLTK.\n",
        "\n",
        "We will make use of the contents of the [Chapter 5](http://www.nltk.org/book/ch05.html) of the\n",
        "[Natural Language Processing with Python --- Analyzing Text with the Natural Language Toolkit](http://www.nltk.org/book). As experimental dataset, we will use the [Brown Corpus](http://en.wikipedia.org/wiki/Brown_Corpus). The Brown Corpus defines a tagset (specific collection of part-of-speech labels) that has been reused in many other annotated resources in English. The [universal tagset](http://universaldependencies.org/u/pos/) includes 17 tags:\n",
        "\n",
        "Tag\t| Meaning\t | Examples\n",
        "----|------------|----------\n",
        "ADJ\t| adjective\t | new, good, high, special, big, local\n",
        "ADV\t| adverb\t | really, already, still, early, now\n",
        "CONJ| conjunction| and, or, but, if, while, although\n",
        "DET\t| determiner | the, a, some, most, every, no\n",
        "X\t| other, foreign words | dolce, ersatz, esprit, quo, maitre\n",
        "NOUN | noun\t     | year, home, costs, time, education\n",
        "PROPN| proper noun | Alison, Africa, April, Washington\n",
        "NUM\t | numeral\t| twenty-four, fourth, 1991, 14:24\n",
        "PRON | pronoun\t| he, their, her, its, my, I, us\n",
        "ADP  | adposition, preposition | on, of, at, with, by, into, under\n",
        "AUX\t | auxiliary verb | has (done), is (doing), will (do), should (do), must (do), can (do)\n",
        "INTJ | interjection | ah, bang, ha, whee, hmpf, oops\n",
        "VERB | verb | is, has, get, do, make, see, run\n",
        "PART | particle | possessive marker 's, negation 'not'\n",
        "SCONJ | subordinating conjunction: complementizer, adverbial clause introducer | I believe 'that' he will come, if, while\n",
        "SYM\t| symbol | $, (C), +, *, /, =, :), john.doe@example.com\n",
        "\n",
        "\n",
        "\n",
        "Note that the decision on how to tag a word, without more information is ambiguous for multiple reasons:\n",
        "\n",
        "- The same string can be understood as a `noun` or a `verb` (e.g, **book**).\n",
        "- Some POS tags have a systematically ambiguous definition: a present participle can be used in progressive verb usages (I am going:VERB), but it can also be used in an adjectival position modifying a noun: (A striking:ADJ comparison). In other words, it is unclear in the definition itself of the tag whether the tag refers to a syntactic function or to a morphological property of the word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqTCmzlHcegJ"
      },
      "source": [
        "## 0. Working on the Brown Corpus with NLTK\n",
        "\n",
        "NLTK contains a collection of tagged corpora, arranged as convenient Python objects. We will use the Brown corpus in this experiment.  The `tagged_sents` version of the corpus is a list of sentences. Each sentence is a list of pairs (`tuples`) `(word, tag)`. Similarly, one can access the corpus as a flat list of tagged words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AxxrfeRcegK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28de5a9-45ef-4788-dee6-6330a431d5cb"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download and import the Brown Corpus\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
        "brown_news_words = brown.tagged_words(categories='news',  tagset='universal')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIlBtbQTcegM"
      },
      "source": [
        "In addition to the `Brown Corpus` we also need to also load the `universal_tagset`, an interface for converting POS tags from various formats to the universal tagset format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uquRKIfqcegN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b420ed1-7e82-4f1a-9afd-45ffcaab4548"
      },
      "source": [
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEZxz2RKcegN"
      },
      "source": [
        "### Measuring success: Accuracy, Training Dataset, Test Dataset\n",
        "\n",
        "Assume we develop a tagger. How do we know how successful it is? Can we trust the decisions the tagger makes? In order to evaluate the tagger, we are going to split the dataset into training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEtM280xcegP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9822c9d2-3280-46c3-c8d1-f5c7701e00b7"
      },
      "source": [
        "brown_train = brown_news_tagged[100:]\n",
        "brown_test = brown_news_tagged[:100]\n",
        "\n",
        "from nltk.tag import untag\n",
        "test_sent = untag(brown_test[0])\n",
        "print(\"Tagged: \", brown_test[0])\n",
        "print()\n",
        "print(\"Untagged: \", test_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tagged:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
            "\n",
            "Untagged:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok_BRio3cegQ"
      },
      "source": [
        "## 1. Baseline Tagger: Default Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKOjVwf4cegR"
      },
      "source": [
        "In the absence of any knowledge, the most basic tagging approach is to assign the same tag to all the words.\n",
        "It can be done with the `DefaultTagger` class, which takes a tag and assign it to all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjX2ZSpKcegS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b28ab1-1544-45b7-9ed2-e42e7a4ae0c8"
      },
      "source": [
        "# A default tagger assigns the same tag to all words\n",
        "from nltk import DefaultTagger\n",
        "default_tagger = DefaultTagger('NOUN')\n",
        "default_tagger.tag('This is a test'.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'NOUN'), ('is', 'NOUN'), ('a', 'NOUN'), ('test', 'NOUN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G66TbF4qcegS"
      },
      "source": [
        "### Exercise 1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OpBFbp8cegT"
      },
      "source": [
        "Using the `DefaultTagger`, try different tags (see the available options in the table at the beginning of the notebook).\n",
        "\n",
        "**Which one is offering the best performance? Why?**\n",
        "\n",
        "To measure success, in this task, we will measure accuracy. The tagger object in NLTK includes a method called `evaluate` to measure the accuracy of a tagger on a given test set (our `brown_test` object).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFvk35GGcegU"
      },
      "source": [
        "Let's try different tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq1R1dP5cegU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a31ee9f-b4cf-4673-ce63-b183704d29a9"
      },
      "source": [
        "# List of tags to try\n",
        "tags = ['ADJ', 'ADV', 'NOUN', 'DET', 'VERB', 'PRON']\n",
        "\n",
        "# Create and evaluate a DefaultTagger for each of the tags\n",
        "for t in tags:\n",
        "    default_tagger = DefaultTagger(t)\n",
        "    print(default_tagger.tag(test_sent))\n",
        "    print('Accuracy for the tag %s: %4.1f%%' % (t, 100.0 * default_tagger.evaluate(brown_test)))\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The', 'ADJ'), ('Fulton', 'ADJ'), ('County', 'ADJ'), ('Grand', 'ADJ'), ('Jury', 'ADJ'), ('said', 'ADJ'), ('Friday', 'ADJ'), ('an', 'ADJ'), ('investigation', 'ADJ'), ('of', 'ADJ'), (\"Atlanta's\", 'ADJ'), ('recent', 'ADJ'), ('primary', 'ADJ'), ('election', 'ADJ'), ('produced', 'ADJ'), ('``', 'ADJ'), ('no', 'ADJ'), ('evidence', 'ADJ'), (\"''\", 'ADJ'), ('that', 'ADJ'), ('any', 'ADJ'), ('irregularities', 'ADJ'), ('took', 'ADJ'), ('place', 'ADJ'), ('.', 'ADJ')]\n",
            "Accuracy for the tag ADJ:  5.3%\n",
            "\n",
            "\n",
            "[('The', 'ADV'), ('Fulton', 'ADV'), ('County', 'ADV'), ('Grand', 'ADV'), ('Jury', 'ADV'), ('said', 'ADV'), ('Friday', 'ADV'), ('an', 'ADV'), ('investigation', 'ADV'), ('of', 'ADV'), (\"Atlanta's\", 'ADV'), ('recent', 'ADV'), ('primary', 'ADV'), ('election', 'ADV'), ('produced', 'ADV'), ('``', 'ADV'), ('no', 'ADV'), ('evidence', 'ADV'), (\"''\", 'ADV'), ('that', 'ADV'), ('any', 'ADV'), ('irregularities', 'ADV'), ('took', 'ADV'), ('place', 'ADV'), ('.', 'ADV')]\n",
            "Accuracy for the tag ADV:  2.6%\n",
            "\n",
            "\n",
            "[('The', 'NOUN'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'NOUN'), ('Jury', 'NOUN'), ('said', 'NOUN'), ('Friday', 'NOUN'), ('an', 'NOUN'), ('investigation', 'NOUN'), ('of', 'NOUN'), (\"Atlanta's\", 'NOUN'), ('recent', 'NOUN'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'NOUN'), ('``', 'NOUN'), ('no', 'NOUN'), ('evidence', 'NOUN'), (\"''\", 'NOUN'), ('that', 'NOUN'), ('any', 'NOUN'), ('irregularities', 'NOUN'), ('took', 'NOUN'), ('place', 'NOUN'), ('.', 'NOUN')]\n",
            "Accuracy for the tag NOUN: 31.8%\n",
            "\n",
            "\n",
            "[('The', 'DET'), ('Fulton', 'DET'), ('County', 'DET'), ('Grand', 'DET'), ('Jury', 'DET'), ('said', 'DET'), ('Friday', 'DET'), ('an', 'DET'), ('investigation', 'DET'), ('of', 'DET'), (\"Atlanta's\", 'DET'), ('recent', 'DET'), ('primary', 'DET'), ('election', 'DET'), ('produced', 'DET'), ('``', 'DET'), ('no', 'DET'), ('evidence', 'DET'), (\"''\", 'DET'), ('that', 'DET'), ('any', 'DET'), ('irregularities', 'DET'), ('took', 'DET'), ('place', 'DET'), ('.', 'DET')]\n",
            "Accuracy for the tag DET: 12.3%\n",
            "\n",
            "\n",
            "[('The', 'VERB'), ('Fulton', 'VERB'), ('County', 'VERB'), ('Grand', 'VERB'), ('Jury', 'VERB'), ('said', 'VERB'), ('Friday', 'VERB'), ('an', 'VERB'), ('investigation', 'VERB'), ('of', 'VERB'), (\"Atlanta's\", 'VERB'), ('recent', 'VERB'), ('primary', 'VERB'), ('election', 'VERB'), ('produced', 'VERB'), ('``', 'VERB'), ('no', 'VERB'), ('evidence', 'VERB'), (\"''\", 'VERB'), ('that', 'VERB'), ('any', 'VERB'), ('irregularities', 'VERB'), ('took', 'VERB'), ('place', 'VERB'), ('.', 'VERB')]\n",
            "Accuracy for the tag VERB: 16.9%\n",
            "\n",
            "\n",
            "[('The', 'PRON'), ('Fulton', 'PRON'), ('County', 'PRON'), ('Grand', 'PRON'), ('Jury', 'PRON'), ('said', 'PRON'), ('Friday', 'PRON'), ('an', 'PRON'), ('investigation', 'PRON'), ('of', 'PRON'), (\"Atlanta's\", 'PRON'), ('recent', 'PRON'), ('primary', 'PRON'), ('election', 'PRON'), ('produced', 'PRON'), ('``', 'PRON'), ('no', 'PRON'), ('evidence', 'PRON'), (\"''\", 'PRON'), ('that', 'PRON'), ('any', 'PRON'), ('irregularities', 'PRON'), ('took', 'PRON'), ('place', 'PRON'), ('.', 'PRON')]\n",
            "Accuracy for the tag PRON:  2.1%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s7sHItUcegV"
      },
      "source": [
        "As you can see, the `DefaultTagger` is giving the same tag to all the words. Since 'NOUN' is the most frequent universal tag in the Brown corpus, it is the one that offers the best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-QmWo0jcegV"
      },
      "source": [
        "## 2. Sources of Knowledge to Improve Tagging Accuracy\n",
        "\n",
        "Intuitively, the sources of knowledge that can help us decide what is the tag of a word include:\n",
        "- A dictionary that lists the possible parts of speech for each word\n",
        "- The context of the word in a sentence (neighboring words)\n",
        "- The morphological form of the word (suffixes, prefixes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAXC3dSQcegW"
      },
      "source": [
        "### 2.1 Lookup Tagger: Using Dictionary Knowledge\n",
        "\n",
        "Assume we have a dictionary that lists the possible tags for each word in English. Could we use this information to perform better tagging?\n",
        "\n",
        "The intuition is that we would only assign to a word a tag that it can have in the dictionary. For example, if `box` can only be a `Verb` or a `Noun`, when we have to tag an instance of the word `box`, we only choose between 2 options - and not between 17 options.\n",
        "\n",
        "There are 3 issues we must address to turn this into working code:\n",
        "\n",
        "- Where do we get the dictionary?\n",
        "- How do we choose between the various tags associated to a word in the dictionary? (For example, how do we choose between `VERB` and `NOUN` for `box`).\n",
        "- What do we do for words that do not appear in the dictionary?\n",
        "\n",
        "The simple solutions we will test are the following - note that for each question, there exist other strategies that we will investigate later:\n",
        "\n",
        "- Where do we get the dictionary?: we will learn it from a sample dataset.\n",
        "- How do we choose between the various tags associated to a word in the dictionary?: we will choose the most likely tag as observed in the sample dataset.\n",
        "- What do we do for words that do not appear in the dictionary?: we will pass unknown words to a backoff tagger (tag all unknown words as `NOUN`).\n",
        "\n",
        "The `nltk.UnigramTagger` implements this overall strategy. It must be trained on a dataset, from which it builds a model of \"unigrams\". The following code shows how it is used:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuhaGabHcegY"
      },
      "source": [
        "### Exercise 2.1.1\n",
        "\n",
        "Use the `UnigramTagger` class and the `brown_train` object to create a unigram tagger.\n",
        "\n",
        "**Which tag is selecting to annotate each word?**\n",
        "\n",
        "**What's happening with unknown words?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Ef7pcHcegY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8019c2f2-5a48-4f0d-8bdd-6f3f68066399"
      },
      "source": [
        "# Prepare training and test datasets\n",
        "from nltk import UnigramTagger\n",
        "\n",
        "# Train the unigram model\n",
        "unigram_tagger = UnigramTagger(brown_train)\n",
        "\n",
        "# Test it on a single sentence\n",
        "unigram_tagger.tag(untag(brown_test[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'),\n",
              " ('Fulton', None),\n",
              " ('County', 'NOUN'),\n",
              " ('Grand', 'ADJ'),\n",
              " ('Jury', 'NOUN'),\n",
              " ('said', 'VERB'),\n",
              " ('Friday', 'NOUN'),\n",
              " ('an', 'DET'),\n",
              " ('investigation', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " (\"Atlanta's\", 'NOUN'),\n",
              " ('recent', 'ADJ'),\n",
              " ('primary', 'NOUN'),\n",
              " ('election', 'NOUN'),\n",
              " ('produced', 'VERB'),\n",
              " ('``', '.'),\n",
              " ('no', 'DET'),\n",
              " ('evidence', 'NOUN'),\n",
              " (\"''\", '.'),\n",
              " ('that', 'ADP'),\n",
              " ('any', 'DET'),\n",
              " ('irregularities', None),\n",
              " ('took', 'VERB'),\n",
              " ('place', 'NOUN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjk8jHs2cegZ"
      },
      "source": [
        "As you can see in the results, the tagger is assigning the most common tag to each word. (e.g., `took` = `VERB`)\n",
        "\n",
        "Note that the unigram tagger leaves some words tagged as `None` -- these are **unknown words**, words that were not observed in the training dataset. We will try to solve that in the following exercises.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkOK0QfYcega"
      },
      "source": [
        "### Exercise 2.1.2\n",
        "\n",
        "Making use of the `evaluate` method measure how successful is this tagger.\n",
        "\n",
        "**Are we improving the performance of the tagger?**\n",
        "**Do your find the new performance sufficient enough for a NLP system?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmv6dU0_cega",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b1ea24-ed58-4000-bec8-8daf2f66168f"
      },
      "source": [
        "print('Unigram tagger accuracy: %4.1f%%' % ( 100.0 * unigram_tagger.evaluate(brown_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram tagger accuracy: 88.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJhPZCXccegb"
      },
      "source": [
        "88.9% is quite an improvement on the 31% of the default tagger.\n",
        "And this is without any backoff and without using morphological clues.\n",
        "\n",
        "Is 88.9% a good level of accuracy? In fact it is not. It is accuracy per word. It means that on average, in every sentence of about 20 words, we will accumulate 2 errors. 2 errors in each sentence is a very high error rate. It makes it difficult to run another task on the output of such a tagger. Think how difficult the life of a parser would be if 2 words in every sentence are wrongly tagged. The problem is known as the **pipeline effect** -- when language processing tools are chained in a pipeline, error rates accumulate from module to module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3yphZRcegb"
      },
      "source": [
        "### Exercise 2.1.3\n",
        "\n",
        "If we analyze the tagger annotation, we will see that it assigns `None` to unknown words. As explained in class, a good way to improve this is to tag unknowns words as `NOUN` (the most common tag). This is known as a backoff tagger (i.e., a second tagger that applies where the original one cannot identify the tag for a word)\n",
        "\n",
        "NLTK provides a simple way to implement this backoff tagging. All the constructors for the Tagger classes (e.g., `UnigramTagger`) have a parameter `backoff` where you can set the backoff tagger that will apply. In this case, our backoff tagger will be the `DefaultTagger` that annotates `NOUN`, which we developed in the exercise below .\n",
        "\n",
        "**Using the `DefaultTagger` and `UnigramTagger` classes, create a tagger that assigns the most common tag to each word and, for unknown words, assigns a backoff tag of `NOUN`.**\n",
        "\n",
        "**What's the accuracy of this tagger? Do we improved our performance?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujTiEezscegc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50e93f0-69de-467d-fe7d-2af7fd31428c"
      },
      "source": [
        "nn_tagger = DefaultTagger('NOUN')\n",
        "ut2 = UnigramTagger(brown_train, backoff=nn_tagger)\n",
        "print('Unigram tagger with backoff accuracy: %4.1f%%' % ( 100.0 * ut2.evaluate(brown_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram tagger with backoff accuracy: 94.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoBtKfMFcegd"
      },
      "source": [
        "Adding a simple backoff (with accuracy of 31%) improved accuracy from 88.9% to 94.5%.\n",
        "\n",
        "The error rate went down from 11.1% (100-88.9) to 5.5%. In other words, out of the words not tagged by the original model (with no backoff), 41.4% were corrected by the backoff.\n",
        "\n",
        "One lesson to learn from this is that the **distribution of unknown words is significantly different from the distribution of all the words in the corpus**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixdJjFZocegd"
      },
      "source": [
        "### 2.2 Using Morphological Clues\n",
        "\n",
        "As mentioned above, another knowledge source to perform tagging is to look at the letter structure of the words.\n",
        "We will look at 2 different methods to use this knowledge.\n",
        "First, we will use nltk.RegexpTagger to recognize specific regular expressions in words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKTKjYGecege",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775b2c2a-557c-466c-c4d6-ea2f7d546485"
      },
      "source": [
        "from nltk import RegexpTagger\n",
        "\n",
        "regexp_tagger = RegexpTagger(\n",
        "     [(r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
        "      (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
        "      (r'.*able$', 'ADJ'),                # adjectives\n",
        "      (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
        "      (r'.*ly$', 'ADV'),                  # adverbs\n",
        "      (r'.*s$', 'NOUN'),                  # plural nouns\n",
        "      (r'.*ing$', 'VERB'),                # gerunds\n",
        "      (r'.*ed$', 'VERB'),                 # past tense verbs\n",
        "      (r'.*', 'NOUN')                     # nouns (default)\n",
        "])\n",
        "\n",
        "print('Regexp accuracy %4.1f%%' % (100.0 * regexp_tagger.evaluate(brown_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regexp accuracy 48.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJtZFdnZcegf"
      },
      "source": [
        "The regular expressions are tested in order. If one matches, it decides the tag. Else it tries the next tag.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTH9AR_Dcegg"
      },
      "source": [
        "The question we face when we see such a \"rule-based\" tagger are:\n",
        "\n",
        "- How do we find the most successful regular expressions?\n",
        "- In which order should we try the regular expressions?\n",
        "\n",
        "A typical answer to such questions is:\n",
        "\n",
        "- let's learn these parameters from a training corpus.\n",
        "\n",
        "The `nltk.AffixTagger` is a trainable tagger that attempts to learn word patterns.\n",
        "It only looks at the last letters in the words in the training corpus, and counts how often a word suffix\n",
        "can predict the word tag.\n",
        "In other words, we only learn rules of the form ('.*xyz' , POS).\n",
        "This is how the affix tagger is used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoVYmVJocegg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d935aae3-0846-4a17-e4ca-5eedf36523b0"
      },
      "source": [
        "from nltk import AffixTagger\n",
        "\n",
        "affix_tagger = AffixTagger(brown_train, backoff=nn_tagger)\n",
        "print('Affix tagger accuracy: %4.1f%%' % (100.0 * affix_tagger.evaluate(brown_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Affix tagger accuracy: 42.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cT8fMjZcegh"
      },
      "source": [
        "Should we be disappointed that the \"data-based approach\" performs worse than the hand-written rules (42% vs. 48%)?\n",
        "\n",
        "Not necessarily: note that our hand-written rules include cases that the AffixTagger cannot learn - we match cardinal numbers and suffixes with more than 3 letters.\n",
        "\n",
        "Let us see whether the combination of the 2 taggers helps:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSkitnYJcegh"
      },
      "source": [
        "### Exercise 2.2.1\n",
        "\n",
        "**Using the `AffixTagger` class, creates a tagger that learns from word patterns and that uses the previous `RegexpTagger` as backoff.**\n",
        "\n",
        "**Evaluate and analyze the performance of the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW5VP557cegi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bf81a3-7389-4893-9f01-79f80138b78b"
      },
      "source": [
        "at2 = AffixTagger(brown_train, backoff=regexp_tagger)\n",
        "print(\"Affix tagger with regexp backoff accuracy: %4.1f%%\" % (100.0 * at2.evaluate(brown_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Affix tagger with regexp backoff accuracy: 52.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4vWAsDCcegi"
      },
      "source": [
        "This is not bad - the machine learning in AffixTagger helped us reduce the error from 52% to 47% (10% error reduction).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHoxvqRgcegj"
      },
      "source": [
        "### Exercise 2.2.2\n",
        "\n",
        "In the previous exercise we created an `AffixTagger` that is able to learn the annotation from the word patterns. Perhaps, we could apply this tagger to annotate the unknown words (i.e., to use it as a backoff tagger). In the previous section, we used a NOUN-default tagger for that. How much does this tagger help the `UnigramTagger` if we use it as a backoff instead of the NOUN-default tagger?\n",
        "\n",
        "**Use the `AffixTagger` that we created below as a backoff tagger for the `UnigramTagger` in the previous section**\n",
        "\n",
        "**Are we improving our tagger?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HC1_xOOcegj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7d1004-6701-4542-ce73-b3629e472542"
      },
      "source": [
        "ut3 = UnigramTagger(brown_train, backoff=at2)\n",
        "print('Unigram with affix backoff accuracy: %4.1f%%' % (100.0 * ut3.evaluate(brown_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram with affix backoff accuracy: 95.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c-Tszuzcegk"
      },
      "source": [
        "The error reduction is from 88.9% to 95.4% -- better that the 94.5% obtained with the NOUN backoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3F-98ltcegk"
      },
      "source": [
        "###  2.3 Looking at the Context\n",
        "\n",
        "At this point, we have combined 2 major sources of information: dictionary and morphology and obtained about 95.4% accuracy. The last source of knowledge we want to exploit the context of the word to be tagged: **the words that appear around the word to be tagged**.\n",
        "\n",
        "The intuition is that if we have to decide between `book` as a verb or a noun, the word/s preceding `book` can give us strong cues: for example, if it is an article (`the` or `a`) then we would be sure that `book` is a noun; if it is `to`, then we would be sure it is a verb.\n",
        "\n",
        "How can we turn this intuition into working code? The easiest way to detect predictive contexts is to construct a list of contexts - and for each context, keep track of the distribution of tags that follow it. Luckily for us, this procedure is already implemented into the `NgramTagger`, which takes a parameter a number setting the length of the context.\n",
        "\n",
        "As usual, if the tagger cannot make a decision (because the observed context was never seen at training time),\n",
        "the decision is delegated to a backoff tagger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIpL0gjgcegk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc1b224-9c14-41ae-bf80-2b8ec60df01a"
      },
      "source": [
        "# Where we stand before looking at the context\n",
        "ut3.evaluate(brown_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9541446208112875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiNG1CIwcegk"
      },
      "source": [
        "### Exercise 2.3.1\n",
        "\n",
        "**Use the `NgramTagger` to create a context-based tagger. For the cases that this tagger cannot annotate anything, use the previous `UnigramTagger` as backoff.**\n",
        "\n",
        "**Try different context sizes (you can set that as a parameter when you create the `NgramTagger`) and analyze how it affects to the final performance of the model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_OP0p0Dcegl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9675839-66e6-47dd-e0cc-a6e953fafc6d"
      },
      "source": [
        "from nltk import NgramTagger\n",
        "\n",
        "ct2 = NgramTagger(2, brown_train, backoff=ut3)\n",
        "ct2.evaluate(brown_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9611992945326279"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgwMTbStcegm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f54e81-feff-4935-c7bb-1ce951384f3a"
      },
      "source": [
        "ct3 = NgramTagger(3, brown_train, backoff=ut3)\n",
        "ct3.evaluate(brown_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9581128747795414"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVsWg80Xcegn"
      },
      "source": [
        "We find on our dataset that looking at a context of 2 tags in the past improves the accuracy from 95.4% to 96.1% -- this is 18% error reduction.\n",
        "\n",
        "If we try to use a larger context of 3 tags, we get less improvement (from 95.4% to 95.8%).\n",
        "\n",
        "The main problem we face is that of **data sparseness**: there are not enough samples of each context for the context to help. We will return to this issue in the next lectures. If we take even larger context sizes, the sparseness will be larger and, consequently, the performance of the tagger smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpu-fIwJcegn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19aedfa-a886-4c93-c73f-118dc10f73b2"
      },
      "source": [
        "ct5 = NgramTagger(5, brown_train, backoff=ut3)\n",
        "ct5.evaluate(brown_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9488536155202821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwwagrBrcego"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This practice introduced tools to tag parts of speech in free text. The key point of the approach we investigated is that it is **data-driven**:\n",
        "\n",
        "- We first define possible knowledge sources that can help us solve the task. Specifically, we investigated\n",
        "  * dictionary,\n",
        "  * morphological\n",
        "  * context\n",
        "  as possible sources.\n",
        "\n",
        "- We tested simple machine learning methods: data is acquired by inspecting a training dataset, then evaluated by testing on a test dataset.\n",
        "\n",
        "- We investigated one method to combine several systems into a combined system: backoff models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txuHRruScego"
      },
      "source": [
        "# Additional Materials: Practical Tagging\n",
        "\n",
        "In this practice we have played with the development of new Taggers. You can refer back to this Notebook if and when you need to create your own Taggers. Nevertheless, most of the time the Taggers already included in the different libraries will do the trick for you.\n",
        "\n",
        "In particular, NLTK provides you a way to tag your dataset with just a couple of lines of code by using the `pos_tag` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY7I8bCAcego"
      },
      "source": [
        "The first thing we need to do is to tokenize the sentence to be tagged. To that end, we can make use of the `word_tokenize` function in NLTK. To perform this tokenization, we need to download an already developed tokenizer model (`punkt`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxRgCWBAcego",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b679b83-b295-4728-fa39-1d75360adc0d"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSCPgdpScegp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3396c8b-f2ec-4ce2-8d47-1f40ef5210eb"
      },
      "source": [
        "text = nltk.word_tokenize(\"And now for something completely different\")\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['And', 'now', 'for', 'something', 'completely', 'different']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1wComOacegp"
      },
      "source": [
        "Then we should feed our tokenized text to the pos tagging function. In this case, we are going to apply one of the already pre-trained pos tagging models included into NLTK: `averaged_perceptron_tagger`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqWSJ-Docegp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc2c1c3-4ebd-4980-ae45-f3e00682431d"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDg8S6RHcegq"
      },
      "source": [
        "Then we should feed our tokenized text to the pos tagging function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1IoI4W_cegq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca2b663-9d5a-4867-efe2-d06a04e577ac"
      },
      "source": [
        "nltk.pos_tag(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('And', 'CC'),\n",
              " ('now', 'RB'),\n",
              " ('for', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('completely', 'RB'),\n",
              " ('different', 'JJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwqtkT-Ncegq"
      },
      "source": [
        "If we have more than one sentence to parse, we can make use of some of the Sentence Tokenizers that nltk provides (e.g. `sent_tokenize`) to split the text in sentences, and the the word tokenizer to split each sentence in words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Y_xE-Icegq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8110dbb-a058-4162-be47-85fe64051a71"
      },
      "source": [
        "sentences = nltk.sent_tokenize(\"And now for something completely different. This is just another sentence\")\n",
        "print(\"Sentences:\", sentences)\n",
        "text = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "print(\"Text:\", text)\n",
        "print()\n",
        "for tagging in [nltk.pos_tag(t) for t in text]:\n",
        "    print(\"Tagging:\",tagging)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences: ['And now for something completely different.', 'This is just another sentence']\n",
            "Text: [['And', 'now', 'for', 'something', 'completely', 'different', '.'], ['This', 'is', 'just', 'another', 'sentence']]\n",
            "\n",
            "Tagging: [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ'), ('.', '.')]\n",
            "Tagging: [('This', 'DT'), ('is', 'VBZ'), ('just', 'RB'), ('another', 'DT'), ('sentence', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w9NvuHrcegr"
      },
      "source": [
        "Let's see a full example with a proper corpus. NLTK provides many corpora that can be used for research or for the training of our NLP system. To find a comprehensive list of all the corpus and how to use them, please refer to the [2nd Chapter of the NLTK book](https://www.nltk.org/book/ch02.html).\n",
        "\n",
        "We will use the corpus `state_union` including the texts of the State of the Union addresses since 1945. Let's load one of these speeches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg9fYvFXcegr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a7f286-aa2f-43fc-d65c-0a3a2e085385"
      },
      "source": [
        "from nltk.corpus import state_union\n",
        "\n",
        "nltk.download('state_union')\n",
        "\n",
        "text = state_union.raw(\"1946-Truman.txt\")\n",
        "print(text[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946.\n",
            " \n",
            "January 21, 1946. Dated January 14, 1946 \n",
            "\n",
            "To the Congress of the United States:\n",
            "A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. Instead it has called on the President to present a comprehensive Executive Budget. The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. The bigger and more complex the Federal Program, the more necessary it is for the Chief Executive to submit a single budget for action by the Congress.\n",
            "At the same time, it is clear that the budgetary program and the general program of the Government are actually inseparable. The President bears the responsibility for recommending to the Congress a comprehensive set of proposals on all Government activities and their financing. In formulating policies, as in\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHdusSdscegs"
      },
      "source": [
        "We now define a function `tag_corpus` that takes care of the tagging process. First, it splits the text in sentences with the `sent_tokenize` function. Then, it iterates over these sentences, tokenize them with the `word_tokenize` function and apply the `pos_tag` function to the tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfDEXnlxcegs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9e3066-0e06-4429-ecf2-f4cab603af1b"
      },
      "source": [
        "def tag_corpus(corpus_text):\n",
        "    try:\n",
        "        for sentence in nltk.sent_tokenize(corpus_text)[:5]: # We just process 5 sentences for the sake of simplicity\n",
        "            words = nltk.word_tokenize(sentence)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            print(\"Sentence:\", sentence, \"\\nTagging:\", tagged)\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "\n",
        "tag_corpus(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946. \n",
            "Tagging: [('PRESIDENT', 'NNP'), ('HARRY', 'NNP'), ('S.', 'NNP'), ('TRUMAN', 'NNP'), (\"'S\", 'POS'), ('MESSAGE', 'NN'), ('TO', 'VBD'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('AND', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('BUDGET', 'NNP'), ('FOR', 'NNP'), ('1946', 'CD'), ('.', '.')]\n",
            "\n",
            "Sentence: January 21, 1946. \n",
            "Tagging: [('January', 'NNP'), ('21', 'CD'), (',', ','), ('1946', 'CD'), ('.', '.')]\n",
            "\n",
            "Sentence: Dated January 14, 1946 \n",
            "\n",
            "To the Congress of the United States:\n",
            "A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. \n",
            "Tagging: [('Dated', 'VBN'), ('January', 'NNP'), ('14', 'CD'), (',', ','), ('1946', 'CD'), ('To', 'TO'), ('the', 'DT'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (':', ':'), ('A', 'DT'), ('quarter', 'NN'), ('century', 'NN'), ('ago', 'IN'), ('the', 'DT'), ('Congress', 'NNP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('no', 'RB'), ('longer', 'RB'), ('consider', 'VB'), ('the', 'DT'), ('financial', 'JJ'), ('programs', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('various', 'JJ'), ('departments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('piecemeal', 'JJ'), ('basis', 'NN'), ('.', '.')]\n",
            "\n",
            "Sentence: Instead it has called on the President to present a comprehensive Executive Budget. \n",
            "Tagging: [('Instead', 'RB'), ('it', 'PRP'), ('has', 'VBZ'), ('called', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('to', 'TO'), ('present', 'VB'), ('a', 'DT'), ('comprehensive', 'JJ'), ('Executive', 'NNP'), ('Budget', 'NNP'), ('.', '.')]\n",
            "\n",
            "Sentence: The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. \n",
            "Tagging: [('The', 'DT'), ('Congress', 'NNP'), ('has', 'VBZ'), ('shown', 'VBN'), ('its', 'PRP$'), ('satisfaction', 'NN'), ('with', 'IN'), ('that', 'DT'), ('method', 'NN'), ('by', 'IN'), ('extending', 'VBG'), ('the', 'DT'), ('budget', 'NN'), ('system', 'NN'), ('and', 'CC'), ('tightening', 'VBG'), ('its', 'PRP$'), ('controls', 'NNS'), ('.', '.')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMc7eM67cegs"
      },
      "source": [
        "Same function in a more *pythonic* way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e92jJG7Rcegt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c77035-d70b-4602-bfec-e373db4760bd"
      },
      "source": [
        "def pythonized_tag_corpus(corpus_text):\n",
        "    try:\n",
        "        [print(\"Sentence:\", sentence, \"\\nTagging:\", nltk.pos_tag(nltk.word_tokenize(sentence)), \"\\n\") for sentence in nltk.sent_tokenize(corpus_text)[:5]]\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "pythonized_tag_corpus(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: PRESIDENT HARRY S. TRUMAN'S MESSAGE TO THE CONGRESS ON THE STATE OF THE UNION AND ON THE BUDGET FOR 1946. \n",
            "Tagging: [('PRESIDENT', 'NNP'), ('HARRY', 'NNP'), ('S.', 'NNP'), ('TRUMAN', 'NNP'), (\"'S\", 'POS'), ('MESSAGE', 'NN'), ('TO', 'VBD'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('AND', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('BUDGET', 'NNP'), ('FOR', 'NNP'), ('1946', 'CD'), ('.', '.')] \n",
            "\n",
            "Sentence: January 21, 1946. \n",
            "Tagging: [('January', 'NNP'), ('21', 'CD'), (',', ','), ('1946', 'CD'), ('.', '.')] \n",
            "\n",
            "Sentence: Dated January 14, 1946 \n",
            "\n",
            "To the Congress of the United States:\n",
            "A quarter century ago the Congress decided that it could no longer consider the financial programs of the various departments on a piecemeal basis. \n",
            "Tagging: [('Dated', 'VBN'), ('January', 'NNP'), ('14', 'CD'), (',', ','), ('1946', 'CD'), ('To', 'TO'), ('the', 'DT'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (':', ':'), ('A', 'DT'), ('quarter', 'NN'), ('century', 'NN'), ('ago', 'IN'), ('the', 'DT'), ('Congress', 'NNP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('could', 'MD'), ('no', 'RB'), ('longer', 'RB'), ('consider', 'VB'), ('the', 'DT'), ('financial', 'JJ'), ('programs', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('various', 'JJ'), ('departments', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('piecemeal', 'JJ'), ('basis', 'NN'), ('.', '.')] \n",
            "\n",
            "Sentence: Instead it has called on the President to present a comprehensive Executive Budget. \n",
            "Tagging: [('Instead', 'RB'), ('it', 'PRP'), ('has', 'VBZ'), ('called', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('President', 'NNP'), ('to', 'TO'), ('present', 'VB'), ('a', 'DT'), ('comprehensive', 'JJ'), ('Executive', 'NNP'), ('Budget', 'NNP'), ('.', '.')] \n",
            "\n",
            "Sentence: The Congress has shown its satisfaction with that method by extending the budget system and tightening its controls. \n",
            "Tagging: [('The', 'DT'), ('Congress', 'NNP'), ('has', 'VBZ'), ('shown', 'VBN'), ('its', 'PRP$'), ('satisfaction', 'NN'), ('with', 'IN'), ('that', 'DT'), ('method', 'NN'), ('by', 'IN'), ('extending', 'VBG'), ('the', 'DT'), ('budget', 'NN'), ('system', 'NN'), ('and', 'CC'), ('tightening', 'VBG'), ('its', 'PRP$'), ('controls', 'NNS'), ('.', '.')] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}