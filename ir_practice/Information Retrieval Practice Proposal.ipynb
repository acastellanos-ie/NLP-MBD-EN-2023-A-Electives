{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Practice\n",
    "\n",
    "Elasticsearch is an open-source distributed search server built on top of Apache Lucene. Itâ€™s a great tool that allows to quickly build applications with full-text search capabilities. The core implementation is in Java, but it provides a nice REST interface which allows to interact with Elasticsearch from any programming language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install elastic search download your the package for your platform from Get Elasticsearch\n",
    " in https://www.elastic.co/es/start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, unzip the tar.gz file and run `bin/elasticsearch` (or `bin\\elasticsearch.bat` on Windows). This will launch the ElasticSearch Server. Once the server is running, by default it's accessible at [localhost:9200](http://localhost:9200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Elastic Search via Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make queries to ElasticSearch you can directly query the server endpoint via REST. However, we can make it easier via the the `elasticsearch-py` Python library. This library provides a wrapper for the REST endpoint that will allow us to query the server form Python.\n",
    "\n",
    "In case you have not yet installed the libraries, you can execute the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install elasticsearch-dsl\n",
    "! pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q, Index\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0: Indexing and Searching Demo for ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run some demo program. In this practice, we will create inverted index of sample documents (indexing) and then use Elasticsearch query grammar to search documents (searching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions\n",
    "\n",
    "Functions to facilitate the reading of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "from collections import namedtuple\n",
    "\n",
    "# A document class with following attributes\n",
    "# filename: document filename\n",
    "# text: body of documment\n",
    "# path: path of document\n",
    "Doc = namedtuple('Doc', 'filename path text')\n",
    "\n",
    "def read_doc(doc_path, encoding):\n",
    "    '''\n",
    "        reads a document from path\n",
    "        input:\n",
    "            - doc_path : path of document\n",
    "            - encoding: encoding\n",
    "        output: =>\n",
    "            - doc: instance of Doc namedtuple\n",
    "    '''\n",
    "    filename = doc_path.split('/')[-1]\n",
    "    fp = io.open(doc_path, 'r', encoding = encoding)\n",
    "    text = fp.read().strip()\n",
    "    fp.close()\n",
    "    return Doc(filename = filename, text = text, path = doc_path)\n",
    "\n",
    "def read_dataset(path, encoding = \"ISO-8859-1\"):\n",
    "    '''\n",
    "        reads multiple documents from path\n",
    "        input:\n",
    "            - doc_path : path of document\n",
    "            - encoding: encoding\n",
    "        output: =>\n",
    "            - docs: instances of Doc namedtuple returned as generator\n",
    "    '''\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for doc_path in files:\n",
    "            yield read_doc(root + '/' + doc_path, encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the connector\n",
    "\n",
    "To index the documents, we first need to make a connection to **Elasticsearch**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn = Elasticsearch(\n",
    "    'localhost',\n",
    "    # sniff before doing anything\n",
    "    sniff_on_start=True,\n",
    "    # refresh nodes after a node fails to respond\n",
    "    sniff_on_connection_fail=True,\n",
    "    # and also every 60 seconds\n",
    "    sniffer_timeout=60\n",
    ")\n",
    "\n",
    "es_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Indexing\n",
    "\n",
    "We will try to index the sample documents in `./sample_documents`. To index the documents, we first need to make a connection to **Elasticsearch**. \n",
    "\n",
    "Before we index the documents, we first need to define the **configuration of elasticsearch**. During this process, you can define basic configuration of indexer such as tokenizer, stemmer, lemmatizer, and also define which search algorithm elasticsearch will use for search.\n",
    "\n",
    "Below code shows a simple configuration settings for this demo.\n",
    "The configuration tells elasticsearch that our document `doc` will have three fields `filename`, `path`, and `text`, and we will use `text` field for search. `my_analyzer` will be used to parse the `text` field, and `my_analyzer` will also be used as a search analyzer, which will parse search queries later on. `index:False` in `filename` and `path` fields tell elasticsearch that we will not index these two fields, therefore, we cannot search these two fields with queries. \n",
    "\n",
    "The detailed documentation of analyzer can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html).\n",
    "\n",
    "`\"similarity\": \"boolean\"` in `text` field will let elasticsearch know that we will use a boolean search algorithm to search `text` field. The detailed documentation of search algorithms can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html)  and [here](https://www.elastic.co/guide/en/elasticsearch/guide/master/search-in-depth.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for indexing\n",
    "settings = {\n",
    "  \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\",\n",
    "          \"analyzer\": \"my_analyzer\",\n",
    "          \"search_analyzer\": \"my_analyzer\"\n",
    "        }\n",
    "      }\n",
    "  },    \n",
    "  \"settings\": {      \n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\"stop\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"whitespace\",\n",
    "          \"char_filter\": [\"my_char_filter\"]\n",
    "        }\n",
    "      },\n",
    "      \"char_filter\": {\n",
    "        \"my_char_filter\": {\n",
    "          \"type\": \"html_strip\",\n",
    "          \"escaped_tags\": [\"b\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve `sample documents` and indexing them into `INDEX_NAME` index. To that end, the following 2 functions will help you in the creation of the index and the indexing of the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ES_HOSTS = ['http://localhost:9200']\n",
    "INDEX_NAME = 'sample_index'\n",
    "DOCS_PATH = 'practice_data/sample_documents'\n",
    "\n",
    "def create_index(es_conn, index_name, settings):\n",
    "    '''\n",
    "        create index structure in elasticsearch server. \n",
    "        If index_name exists in the server, it will be removed, and new index will be created.\n",
    "        input:\n",
    "            - es_conn: elasticsearch connection object\n",
    "            - index_name: name of index to create\n",
    "            - settings: settings and mappings for index to create\n",
    "        output: =>\n",
    "            - None\n",
    "    '''\n",
    "    if es_conn.indices.exists(index_name):\n",
    "        es_conn.indices.delete(index = index_name)\n",
    "        print('index `{}` deleted'.format(index_name))\n",
    "    es_conn.indices.create(index = index_name, body = settings)\n",
    "    print('index `{}` created'.format(index_name))            \n",
    "            \n",
    "def build_index(es_conn, dataset, index_name, settings, DOC_TYPE='doc'):\n",
    "    '''\n",
    "        build index from a collection of documents\n",
    "        input:\n",
    "            - es_conn: elasticsearch connection object\n",
    "            - dataset: iterable, collection of namedtuple Doc objects\n",
    "            - index_name: name of the index where the documents will be stored\n",
    "            - DOC_TYPE: type signature of documents\n",
    "    '''\n",
    "    # create the index if it doesn't exist\n",
    "    create_index(es_conn = es_conn, index_name = index_name, settings=settings)\n",
    "    counter_read, counter_idx_failed = 0, 0 # counters\n",
    "\n",
    "    # retrive & index documents\n",
    "    for doc in dataset:\n",
    "        res = es_conn.index(\n",
    "            index = index_name,\n",
    "            id = doc.filename,\n",
    "            body = doc._asdict())\n",
    "        counter_read += 1\n",
    "\n",
    "        if res['result'] != 'created':\n",
    "            conter_idx_failed += 1\n",
    "        elif counter_read % 500 == 0:\n",
    "            print('indexed {} documents'.format(counter_read))\n",
    "\n",
    "    print('indexed {} docs to index `{}`, failed to index {} docs'.format(\n",
    "        counter_read,\n",
    "        index_name,\n",
    "        counter_idx_failed\n",
    "    ))\n",
    "    \n",
    "    # refresh after indexing\n",
    "    es_conn.indices.refresh(index=index_name)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset(DOCS_PATH)\n",
    "build_index(es_conn, dataset, INDEX_NAME, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully created an inverted index for the sample documents in `./sample/documents`. It's time to search the documents with some queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Text Search\n",
    "\n",
    "The two most important aspects of full-text search are as follows:\n",
    "\n",
    "##### Relevance\n",
    "\n",
    ">The ability to rank results by how relevant they are to the given query, whether relevance is calculated using TF/IDF (see [What Is Relevance?](https://www.elastic.co/guide/en/elasticsearch/guide/master/relevance-intro.html)), proximity to a geolocation, fuzzy similarity, or some other algorithm.\n",
    "\n",
    "##### Analysis\n",
    "\n",
    ">The process of converting a block of text into distinct, normalized tokens (see [Analysis and Analyzers](https://www.elastic.co/guide/en/elasticsearch/guide/master/analysis-intro.html) in order to (a) create an inverted index and (b) query the inverted index.\n",
    "\n",
    "#### Term-Based Versus Full-Text\n",
    "\n",
    "Two types of text query:\n",
    "\n",
    "##### Term-based\n",
    "\n",
    "Queries like the term or fuzzy queries are low-level queries that have no analysis phase. They operate on a single term. A term query for the term Foo looks for that exact term in the inverted index and calculates the TF/IDF relevance _score for each document that contains the term.\n",
    "\n",
    "##### Full-text queries\n",
    "\n",
    "Queries like the match or query_string queries are high-level queries that understand the mapping of a field:\n",
    "\n",
    "* If you use them to query a date or integer field, they will treat the query string as a date or integer, respectively.\n",
    "\n",
    "* If you query an exact value (not_analyzed) string field, they will treat the whole query string as a single term.\n",
    "\n",
    "* But if you query a full-text (analyzed) field, they will first pass the query string through the appropriate analyzer to produce the list of terms to be queried.\n",
    "\n",
    "Once the query has assembled a list of terms, it executes the appropriate low-level query for each of these terms, and then combines their results to produce the final relevance score for each document.\n",
    "\n",
    "#### The match Query\n",
    "\n",
    "We will perform now different types of queries.\n",
    "\n",
    "First, a query with a single term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=es_conn, index=\"sample_index\")\n",
    "s = s.query(\"match\", text={\"query\": \"obama\"})\n",
    "res = s.execute()\n",
    "\n",
    "for hit in res:\n",
    "    print(hit.filename, hit.text[:100], '... - Score:', hit.meta.score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Multiword Queries\n",
    "\n",
    "Obviously, we can search on more than one word at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1.txt Barack Hussein Obama II (born August 4, 1961) is the 44th and current President of the United States, the first African American to hold the office. He served as the junior United States Senator from  ... - Score: 2.0\n",
      "\n",
      "doc2.txt Michelle LaVaughn Robinson Obama (born January 17, 1964) is the wife of the forty-fourth President of the United States, Barack Obama, and is the first African-American First Lady of the United States ... - Score: 1.0\n",
      "\n",
      "doc3.txt Joseph Robinette \"Joe\" Biden, Jr. (born November 20, 1942) is the 47th and current Vice President of the United States. He was a United States Senator from Delaware from January 3, 1973 until his resi ... - Score: 1.0\n",
      "\n",
      "doc4.txt Hillary Diane Rodham Clinton (born October 26, 1947) is the 67th United States Secretary of State, serving within the administration of President Barack Obama. She was a United States Senate from New  ... - Score: 1.0\n",
      "\n",
      "doc5.txt John Sidney McCain III (born August 29, 1936) is the senior United States Senator from Arizona. He was the Republican nominee for president in the 2008 United States election.\n",
      "\n",
      "McCain followed his fat ... - Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=es_conn, index=\"sample_index\")\n",
    "s = s.query(\"match\", text={\"query\":    \"Obama Hillary\"})\n",
    "res = s.execute()\n",
    "\n",
    "for hit in res:\n",
    "    print(hit.filename, hit.text[:200], '... - Score:', hit.meta.score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing is: any document whose title field contains at least one of the specified terms will match the query. The more terms that match, the more relevant the document.\n",
    "\n",
    "But what happens if I want both terms appearing in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1.txt Barack Hussein Obama II (born August 4, 1961) is the 44th and current President of the United States, the first African American to hold the office. He served as the junior United States Senator from Illinois from January 2005 until he resigned after his election to the presidency in November 2008.\n",
      "\n",
      "Obama is a graduate of Columbia University and Harvard Law School, where he was the president of the Harvard Law Review. He was a community organizer in Chicago before earning his law degree. He worked as a civil rights attorney in Chicago and also taught constitutional law at the University of Chicago Law School from 1992 to 2004.\n",
      "\n",
      "Obama served three terms in the Illinois Senate from 1997 to 2004. Following an unsuccessful bid for a seat in the U.S. House of Representatives in 2000, Obama ran for United States Senate in 2004. His victory, from a crowded field, in the March 2004 Democratic primary raised his visibility. His prime-time televised keynote address at the Democratic National Convention in July 2004 made him a rising star nationally in the Democratic Party. He was elected to the U.S. Senate in November 2004 by the largest margin in the history of Illinois.\n",
      "\n",
      "He began his run for the presidency in February 2007. After a close campaign in the 2008 Democratic Party presidential primaries against Hillary Rodham Clinton, he won his party's nomination, becoming the first major party African American candidate for president. In the 2008 general election, he defeated Republican nominee John McCain and was inaugurated as president on January 20, 2009. ... - Score: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=es_conn, index=\"sample_index\")\n",
    "s = s.query(\"match\", text={\n",
    "    \"query\":    \"Obama Hillary\",\n",
    "    \"operator\": \"and\"})\n",
    "res = s.execute()\n",
    "\n",
    "for hit in res:\n",
    "    print(hit.filename, hit.text, '... - Score:', hit.meta.score)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now containing a term but NOT the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc4.txt Hillary Diane Rodham Clinton (born October 26, 1947) is the 67th United States Secretary of State, s ... - Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boolean Query \"Obama BUT Hillary\"\n",
    "s = Search(using=es_conn, index=\"sample_index\")\n",
    "s = s.query(\"bool\", \n",
    "            must = [Q('match', text=\"hillary\")],\n",
    "            must_not = [Q('match', text=\"obama\")]\n",
    "           )\n",
    "\n",
    "res = s.execute()\n",
    "\n",
    "for hit in res:\n",
    "    print(hit.filename, hit.text[:100], '... - Score:', hit.meta.score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 1: Evaluating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show how the retrieved result can be evaluated by **trec_eval** evaluation program.\n",
    "\n",
    "**trec_eval** is the standard software for evaluating search engines with test collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREC_EVAL setup\n",
    "\n",
    "First, we need to install `trec_eval`. To install\n",
    "\n",
    "- unzip `trec_eval-master.zip`\n",
    "- go to `trec_eval-master` folder\n",
    "- run shell command `make` to create `trec_eval` binary file (If your are using Windows, you can install `make` from [here](http://gnuwin32.sourceforge.net/packages/make.htm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the `government` folder which contains three things:\n",
    "\n",
    "- A set of documents needed to be indexed, in the *documents* directory.\n",
    "    \n",
    "- A set of queries, also called 'topics', in *topics/gov.topics* file. The format of **.topic* file is \"query_id query_terms\". For example, the first line of 'air.topics' file is\n",
    "    \n",
    "    `1 mining gold silver coal`\n",
    "    \n",
    "    which means that the ID of query is *01* and the corresponding query is *mining gold silver coal*.\n",
    "\n",
    "- A set of judgements, saying which documents are relevant for each query, in the *qrels/gov.qrels* file. The format of **.qrels* file is \"query_id 0 document_name binary_relevance\". For example, the first line of 'air.qrels' is\n",
    "    \n",
    "    `1 0 G00-00-0681214 0`\n",
    "    \n",
    "    which means that the document `G00-00-0681214` is not relevant to the given query id *01*. The binary relevance is *1* if the file is relevant to the query, otherwise *0*. Please ignore the second argument *0* as it is always *0*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new index\n",
    "\n",
    "In the previous exercise, we have created the index (inverted-index) of five sample documents. In this one, you will create a new index with the documents in `government/documents` folder .\n",
    "\n",
    "To build a new index, you first need to create a new index. Note that `EVAL_INDEX_NAME` should be changed in order to build separate index for the documents in `government/documents`.\n",
    "\n",
    "After creating the new configuration file, now your job is to create the new index reusing the code in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "  \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\",\n",
    "          \"analyzer\": \"my_analyzer\",\n",
    "          \"search_analyzer\": \"my_analyzer\"\n",
    "        }\n",
    "      }\n",
    "  },    \n",
    "  \"settings\": {      \n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"stop\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"html_strip\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"whitespace\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Create the new index\n",
    "\n",
    "You can reuse the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_HOSTS = ['http://localhost:9200']\n",
    "EVAL_INDEX_NAME = 'government'\n",
    "EVAL_DOCS_PATH = 'practice_data/government/documents'\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2. Read topics and produce result file\n",
    "\n",
    "Read topics (queries) from a file (`government/topics/gov.topics`) and then search documents indexed by **Elasticsearch**. You may choose one of search algorithms.\n",
    "\n",
    "Produce result file (e.g., *retrieved.txt*) according to **trec_eval** standard output format: \n",
    "\n",
    "`01 Q0 document1 0 1.23 my_IR_system1`\n",
    "\n",
    "`01 Q0 document2 1 1.08 my_IR_system1`\n",
    "\n",
    "where '01' is the query ID; ignore 'Q0'; 'documentX' is the name of the file; '0' (or '1' or some other integer number) is the rank of this result; '1.23' (or '1.08' or some other number) is the score of this result; and 'my_IR_system1' is the name for your retrieval system. In particular, note that the rank field will be ignored in **trec_eval**; internally ranks are assigned by sorting by the score field with ties broken deterministicly (using file name).\n",
    "\n",
    "**Now here's your first job**\n",
    "\n",
    "1. read `gov.topics` file line by line, \n",
    "2. send query to the elastic search\n",
    "3. write output according the the output format described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3.  Evaluation\n",
    "\n",
    "It's time to run the evaluation which compares the qrels file provided in *gov.qrels* with your result file.\n",
    "\n",
    "TREC_EVAL is an initiative to evaluate the performance of your search engine. To evaluate your search result, you first need two sets of files: the retrieved result file and the ground truth file.\n",
    "Let's say your retrieval result is saved at `retrieved.txt`, and the ground truth file is saved at `gov.qrels`. \n",
    "\n",
    "The TREC_EVAL evaluation tool is rather outdated and difficult to execute. For this reason, I have taken the following piece of code from this repository https://github.com/prachibhansali/TrecIREvaluation to facilitate its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import getopt\n",
    "\n",
    "def computePrecisionsAndRecall(rankedDocs, queryRelevantDocs):\n",
    "\tprecisions = defaultdict(list)\n",
    "\trecall = defaultdict(list)\n",
    "\tkprecisions = defaultdict(list)\n",
    "\tkrecall = defaultdict(list)\n",
    "\tfValues = defaultdict(list)\n",
    "\trPrecisions = {}\n",
    "\tfor key, value in rankedDocs.items():\n",
    "\t\trel=0\n",
    "\t\tqlen = len(queryRelevantDocs[key])\n",
    "\t\tfor index, docid in enumerate(value):\n",
    "\t\t\tif(docid in queryRelevantDocs[key]):\n",
    "\t\t\t\trel=rel+1\n",
    "\t\t\tprecision_index=float(rel)/(index+1)\n",
    "\t\t\trecall_index=float(rel)/qlen\n",
    "\t\t\tif(docid in queryRelevantDocs[key]):\n",
    "\t\t\t\tprecisions[key].append(precision_index)\n",
    "\t\t\t\trecall[key].append(recall_index)\n",
    "\t\t\tif((index+1) in kranks):\n",
    "\t\t\t\tkprecisions[key].append((index+1,precision_index))\n",
    "\t\t\t\tkrecall[key].append((index+1,recall_index))\n",
    "\t\t\t\tfval = computeFValue(precision_index , recall_index)\n",
    "\t\t\t\tfValues[key].append((index+1,fval))\n",
    "\t\t\tif((index+1)==len(queryRelevantDocs[key])):\n",
    "\t\t\t\trPrecisions[key]=precision_index\n",
    "\treturn precisions, recall, rPrecisions, kprecisions, krecall, fValues\n",
    "\n",
    "def computeAveragePrecision(precisions, queryRelevantDocs):\n",
    "\tavgPrecisions = {}\n",
    "\tfor key,value in precisions.items():\n",
    "\t\tsum=0\n",
    "\t\tfor f in value:\n",
    "\t\t\tsum = sum + float(f)\n",
    "\t\tsum = sum/(len(queryRelevantDocs[key]))\n",
    "\t\tavgPrecisions[key]=sum\n",
    "\treturn avgPrecisions\n",
    "\n",
    "def computeFValue(p,r):\n",
    "\tif(p==0 and r==0):\n",
    "\t\treturn 0\n",
    "\treturn (2*float(p)*float(r))/(float(p)+float(r))\n",
    "\n",
    "import math\n",
    "def computeNDCG(rankedDocs, queryRelevantDocs, grades):\n",
    "\tndcg = {}\n",
    "\tfor key,value in rankedDocs.items():\n",
    "\t\tsum=0\n",
    "\t\tfor index,docid in enumerate(value):\n",
    "\t\t\tif(docid in queryRelevantDocs[key]):\n",
    "\t\t\t\trank = index+1;\n",
    "\t\t\t\tids = [id for id in grades[key] if id[0] == docid]\n",
    "\t\t\t\ttup = ids[0]\n",
    "\t\t\t\tgrade = tup[1]\n",
    "\t\t\t\tsum = sum + (((2**grade)-1) * (1/(math.log((1+rank),2))))\n",
    "\t\tndcg[key] = sum\n",
    "\treturn ndcg\n",
    "\n",
    "def computeKAverageAllQueries(lst, kranks):\n",
    "\tkp = []\n",
    "\tfor rank in kranks:\n",
    "\t\tsum=0\n",
    "\t\tfor key,values in lst.items():\n",
    "\t\t\tscores = [score for score in values if score[0]==rank]\n",
    "\t\t\tfor score in (item[1] for item in scores):\n",
    "\t\t\t\tsum = sum + score\n",
    "\t\tsum = sum/(len(lst))\n",
    "\t\tkp.append(sum)\n",
    "\treturn kp\n",
    "\n",
    "def computeAverageAllQueries(lst, avgPrecisions):\n",
    "\tavg=0\n",
    "\tfor _,v in lst.items():\n",
    "\t\tavg = avg+v\t\n",
    "\treturn float(avg)/len(avgPrecisions)\n",
    "\n",
    "def evaluate(hasQ,qrel_loc,rankedlist_loc):\n",
    "    \n",
    "\tgrades = defaultdict(list)\n",
    "\tkranks = [5,10,20,50,100]\n",
    "\tqueryRelevantDocs = defaultdict(list)\n",
    "\trankedDocs = defaultdict(list)\n",
    "    \n",
    "\twith open(qrel_loc) as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\t(qid,_,docid,rel) = line.split(' ')\n",
    "\t\t\tif(int(rel)==1 or int(rel)==2):\n",
    "\t\t\t\tqueryRelevantDocs[int(qid)].append(docid)\n",
    "\t\t\t\tgrades[int(qid)].append((docid,int(rel)))\n",
    "\n",
    "\twith open(rankedlist_loc) as r:\n",
    "\t\tfor line in r:\n",
    "\t\t\t(qid,_,docid,_,score,_) = line.split(' ')\n",
    "\t\t\tt=(docid,float(score))\n",
    "\t\t\trankedDocs[int(qid)].append(t)\n",
    "\n",
    "\timport operator\n",
    "\tfor qid, value in rankedDocs.items():\n",
    "\t\tvalue.sort(key=operator.itemgetter(1),reverse=True)\n",
    "\t\trankedDocs[qid]=list(x[0] for x in value)\n",
    "\n",
    "\tprecisions, recall, rPrecisions, kprecisions, fValues, krecall = computePrecisionsAndRecall(rankedDocs, queryRelevantDocs)\n",
    "\tavgPrecisions = computeAveragePrecision(precisions, queryRelevantDocs)\n",
    "\tndcg = computeNDCG(rankedDocs, queryRelevantDocs, grades)\n",
    "\n",
    "\tif(hasQ==True):\n",
    "\t\tavgPrecisionAllQueries = computeAverageAllQueries(avgPrecisions, avgPrecisions)\n",
    "\t\tavgRPrecisionAllQueries = computeAverageAllQueries(rPrecisions, avgPrecisions)\n",
    "\t\tavgndcgAllQueries = computeAverageAllQueries(ndcg, avgPrecisions)\n",
    "\t\trt,rl,retrel,pvd,rvd,fvd = calculate_avg_metrics(avgPrecisions,rankedDocs,queryRelevantDocs,kprecisions,krecall,fValues)\n",
    "\twriteAverageOverQueries(rt,rl,retrel,pvd,rvd,fvd,avgPrecisionAllQueries,avgRPrecisionAllQueries,avgndcgAllQueries,kranks,kprecisions)\n",
    "\treturn\n",
    "\n",
    "def calculate_avg_metrics(avgPrecisions,rankedDocs,queryRelevantDocs,kprecisions,krecall,fValues):\n",
    "\tretrievedDocs=0\n",
    "\trelevantDocs=0\n",
    "\tretrel = 0\n",
    "\tpvd = {}\n",
    "\trvd = {}\n",
    "\tfvd = {}\n",
    "\t\n",
    "\tfor q in avgPrecisions:\n",
    "\t\tretrievedDocs = retrievedDocs + len(rankedDocs[q])\n",
    "\t\trelevantDocs = relevantDocs + len(queryRelevantDocs[q])\n",
    "\t\trelev = set(queryRelevantDocs[q])\n",
    "\t\tretr = set(rankedDocs[q])\n",
    "\t\tretrel = retrel+len(relev.intersection(retr))\n",
    "\t\tk=0\n",
    "\t\tfor pv,rv,fv in zip(kprecisions[q],krecall[q],fValues[q]):\n",
    "\t\t\tval = pvd[k] if k in pvd else 0 \n",
    "\t\t\tpvd[k]=val+pv[1]\n",
    "\t\t\tval = rvd[k] if k in rvd else 0 \n",
    "\t\t\trvd[k]=val+rv[1]\n",
    "\t\t\tval = fvd[k] if k in fvd else 0 \n",
    "\t\t\tfvd[k]=val+fv[1]\n",
    "\t\t\tk=k+1\n",
    "\treturn (retrievedDocs,relevantDocs,retrel,pvd,rvd,fvd)\n",
    "\n",
    "def writeAverageOverQueries(rt,rl,retrel,pvd,rvd,fvd,avgPrecisionAllQueries,avgRPrecisionAllQueries,avgndcgAllQueries,kranks,kprecisions):\n",
    "\tprint('Total number of documents')\n",
    "\tprint('Retrieved : '+str(rt))\n",
    "\tprint('Relevant : '+str(rl) )\n",
    "\tprint('ret_rel : ' + str(retrel))\n",
    "\tprint('Average precision (non-interpolated) for all rel docs(averaged over queries): ' + \"%.2f\" %(avgPrecisionAllQueries))\n",
    "\tprint('K'+ '\\t' +'Precision'+'\\t'+'Recall'+'\\t\\t'+'F1')\n",
    "\twriteKValues(pvd,rvd,fvd,kranks,kprecisions)\n",
    "\tprint('R-Precision (precision after R (= num_rel for a query) docs retrieved):' + \"%.2f\" %(avgRPrecisionAllQueries) )\n",
    "\tprint('ndcg over all queries : ' + \"%.2f\" %(avgndcgAllQueries) )\n",
    "\treturn\n",
    "\n",
    "def writeKValues(pvd,rvd,fvd,kranks,kprecisions):\n",
    "\tk=0\n",
    "\tlength = len(kprecisions)\n",
    "\tfor r in kranks:\n",
    "\t\tprint(str(r)+\"\\t\"+\"%.2f\" % (float(pvd[k])/length)+\"\\t\\t\"+\"%.2f\" %(float(rvd[k])/length)+\"\\t\\t\"+\"%.2f\" %(float(fvd[k])/length))\n",
    "\t\tk=k+1\n",
    "\treturn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline retrieval that we have proposed before did offer a rather low performance. In order to improve it, we can tune the index setting to include some of the NLP processing that we have learned (e.g., stemming, stopwords, ...)-\n",
    "\n",
    "To that end, review the documentation of analyzer [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could generate our own analyzers (as we did in the previous exercises with `my_analyzer`), Elasticsearch provides a set of predefined analyzers for the different languages. More information [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html).\n",
    "\n",
    "In particular, we are going to use the `English Analyzer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can modify the index to use a more sophisticated similarity measure (e.g., `BM25`) than the binary similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 English Analyzer + BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the settings to apply the `English Analyzer` and use the `BM25` similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_settings = {  \n",
    "# Your code here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new settings we will create a new index, generate a new result file and evaluate it by means of the `trec_eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_HOSTS = ['http://localhost:9200']\n",
    "EVAL_INDEX_NAME = 'government'\n",
    "EVAL_DOCS_PATH = 'practice_data/government/documents'\n",
    "\n",
    "es_conn = Elasticsearch(ES_HOSTS)\n",
    "dataset = read_dataset(EVAL_DOCS_PATH)\n",
    "build_index(es_conn, dataset, EVAL_INDEX_NAME, new_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"improved_retrieved.txt\",\"w+\")\n",
    "\n",
    "es_conn = Elasticsearch(ES_HOSTS)\n",
    "for query_id, query in queries:\n",
    "    res = search(query, es_conn, EVAL_INDEX_NAME)\n",
    "    write_trec_file(query_id, res, output_file)\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(True,\"./practice_data/government/qrels/gov.qrels\",\"improved_retrieved.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
