{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/EtkQeoIaJB46DwpRTypz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBD-EN-2023-A-Electives/blob/main/qa_practice_dl/LLama2_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Creation with Gradio and HuggingFace\n",
        "\n",
        "In this practice notebook, we will embark on an exciting journey to create an interactive chatbot. Our chatbot will be powered by [Llama 2](https://ai.meta.com/resources/llama-2/), an open-source large language model by Meta AI, and we'll be building an interactive user interface using [Gradio](https://gradio.app/). This combination allows us to leverage state-of-the-art natural language processing (NLP) models in a user-friendly chat interface.\n",
        "\n",
        "This practice is based on the following blog: [How to Create LLaMa 2 Chatbot with Gradio and Hugging Face in Free Colab](https://pub.towardsai.net/how-to-create-llama-2-chatbot-with-gradio-and-hugging-face-in-free-colab-729fe4fb5734)\n",
        "\n",
        "## Overview of Tools\n",
        "- **[HuggingFace](https://huggingface.co/)**: A platform dedicated to hosting and sharing transformer models. We'll be selecting and utilizing a pre-trained LLM from HuggingFace's vast Model Hub.\n",
        "- **[Gradio](https://gradio.app/)**: An easy-to-use library for creating interactive machine learning demos with just a few lines of code. It will serve as the interface through which users can interact with our chatbot.\n",
        "- **[Llama 2](https://ai.meta.com/resources/llama-2/)**: The latest version of Meta's large language model trained on 2 trillion tokens, available for both research and commercial use\n",
        "\n",
        "## Objectives\n",
        "- Select and load a pre-trained LLM from HuggingFace.\n",
        "- Create a function to process user inputs and generate chatbot responses using the LLM.\n",
        "- Set up a Gradio interface for interactive user engagement with the chatbot.\n",
        "- Test, iterate, and refine the chatbot to improve its performance and user experience.\n",
        "\n",
        "By the end of this practice, you will have a functioning chatbot that you can interact with in real-time, and share with others. Now, let's dive into the initial setup and start building our chatbot!\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dh-dQaZbj1Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Imports\n",
        "Before diving into the code, we need to import the essential libraries. We'll use `gradio` for creating the interactive interface and `transformers` to access the Llama 2 model and tokenizer."
      ],
      "metadata": {
        "id": "T9PKfsvqj-xb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VF7u1ZISRu3"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq transformers torch accelerate\n",
        "!pip install -Uqqq --upgrade gradio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authentication with Hugging Face\n",
        "\n",
        "Before we can proceed with creating our chatbot, it's essential to authenticate ourselves with Hugging Face. Hugging Face hosts a plethora of pre-trained models and datasets which will be invaluable for our project. By logging in, we ensure a seamless interaction with the Hugging Face platform, allowing us to fetch necessary resources for our chatbot.\n",
        "\n",
        "Run the command below, you'll be prompted to enter your Hugging Face credentials, or to create an account ([Create a HuggingFace account](https://hf.co/join)) if you don't have one already.\n"
      ],
      "metadata": {
        "id": "Qxm2RV7vkjuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "xirraUWASw4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the LLaMa 2 Model and Tokenizer\n",
        "\n",
        "In this step, we are going to load the LLaMa 2 model along with its tokenizer using the `transformers` library. The model we are utilizing is `meta-llama/Llama-2-7b-chat-hf`, which is hosted on Hugging Face's model hub.\n",
        "\n",
        "1. **Model**: The chosen model, `meta-llama/Llama-2-7b-chat-hf`, is a large language model trained specifically for chat-based interactions. It will serve as the core engine for generating responses to user queries in our chatbot.\n",
        "2. **Tokenizer**: Tokenizers are crucial for preparing our text data into a format that the model can understand. In this case, we are using `AutoTokenizer` which will automatically select the correct tokenizer based on the model we specify.\n",
        "\n",
        "The `use_auth_token=True` argument in `AutoTokenizer.from_pretrained` function ensures that we are authenticated with Hugging Face, allowing us to access and download the model and tokenizer seamlessly.\n",
        "\n",
        "**Be aware that this code might take a while since it needs to download the model from HuggingFace**\n",
        "\n",
        "**You might need a Pro Colab account for running this code**"
      ],
      "metadata": {
        "id": "WpVHu1B-l_Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "k6CLDsY5S6JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the LLaMa Pipeline\n",
        "\n",
        "Now that we have loaded the LLaMa 2 model and tokenizer, the next step is to set up a pipeline for text generation. The `pipeline` function from the `transformers` library provides a high-level, easy to use API for applying transformers to various tasks. In our case, we are interested in the `text-generation` task as we aim to generate responses for our chatbot.\n"
      ],
      "metadata": {
        "id": "FJE2mip1mJ7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "EwhpeRRoTBtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Input Data for LLaMa Model\n",
        "\n",
        "In order to effectively interact with the LLaMa model, we need to format the input data in a specific manner. This includes crafting a system prompt and formatting the conversation history along with the current message to fit the model's expectations. Here's what the code does:\n",
        "\n",
        "1. **Defining System Prompt**:\n",
        "    - We create a constant `SYSTEM_PROMPT` that sets the stage for the interaction, instructing the model to behave as a helpful bot providing clear and concise answers.\n",
        "\n",
        "2. **Formatting Function**:\n",
        "    - The `format_message` function is designed to format the current message and past conversation history into a string that can be fed to the LLaMa model.\n",
        "    - Parameters:\n",
        "        - `message (str)`: The current message to send to the model.\n",
        "        - `history (list)`: A list containing past interactions.\n",
        "        - `memory_limit (int)`: A limit on how many past interactions to consider (default is 3).\n",
        "\n",
        "    - The function ensures that the history is within the specified `memory_limit`.\n",
        "    - If the conversation history is empty, it simply attaches the current message to the `SYSTEM_PROMPT`.\n",
        "    - If there's existing conversation history, it formats the history and the current message together in a way that the model can understand the context of the conversation.\n",
        "\n",
        "For more details, please refer to the original [blog post](https://pub.towardsai.net/how-to-create-llama-2-chatbot-with-gradio-and-hugging-face-in-free-colab-729fe4fb5734)"
      ],
      "metadata": {
        "id": "4NM-XKbqm69a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful bot. Your answers are clear and concise.\n",
        "<</SYS>>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Formatting function for message and history\n",
        "def format_message(message: str, history: list, memory_limit: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Formats the message and history for the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        message (str): Current message to send.\n",
        "        history (list): Past conversation history.\n",
        "        memory_limit (int): Limit on how many past interactions to consider.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted message string\n",
        "    \"\"\"\n",
        "    # always keep len(history) <= memory_limit\n",
        "    if len(history) > memory_limit:\n",
        "        history = history[-memory_limit:]\n",
        "\n",
        "    if len(history) == 0:\n",
        "        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n",
        "\n",
        "    formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n",
        "\n",
        "    # Handle conversation history\n",
        "    for user_msg, model_answer in history[1:]:\n",
        "        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n",
        "\n",
        "    # Handle the current message\n",
        "    formatted_message += f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    return formatted_message"
      ],
      "metadata": {
        "id": "ao3gSREWTaDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Responses with the LLaMa Model\n",
        "\n",
        "Now that we have structured the input data, the next step is to define a function to generate conversational responses from the LLaMa model based on the user's input and the conversation history.\n",
        "\n",
        "Here's a breakdown of the `get_llama_response` function:\n",
        "\n",
        "1. **Function Definition**:\n",
        "    - `get_llama_response` is defined with two parameters: `message` (the user's input message) and `history` (the past conversation history).\n",
        "    - The function returns the generated response from the LLaMa model as a string.\n",
        "\n",
        "2. **Formatting the Input**:\n",
        "    - The `format_message` function is called to format the `message` and `history` into a string `query` that can be fed to the LLaMa model.\n",
        "\n",
        "3. **Generating a Response**:\n",
        "    - The `llama_pipeline` is called with several parameters to control the text generation process:\n",
        "        - `do_sample=True`: Enables stochastic sampling during generation.\n",
        "        - `top_k=10`: Limits the sampling pool to the top 10 most probable tokens at each step.\n",
        "        - `num_return_sequences=1`: Specifies that only one sequence should be returned.\n",
        "        - `eos_token_id=tokenizer.eos_token_id`: Specifies the End Of Sentence (EOS) token ID to indicate the end of generation.\n",
        "        - `max_length=1024`: Sets a maximum length for the generated sequence.\n",
        "\n",
        "4. **Extracting the Generated Text**:\n",
        "    - The generated text is extracted from the `sequences` object, and the prompt (`query`) is removed from the beginning of the generated text to isolate the model's response.\n",
        "\n",
        "5. **Outputting and Returning the Response**:\n",
        "    - The generated response is printed to the console and returned by the function.\n",
        "\n"
      ],
      "metadata": {
        "id": "-89Qt_Ztoydx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response from the Llama model\n",
        "def get_llama_response(message: str, history: list) -> str:\n",
        "    \"\"\"\n",
        "    Generates a conversational response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        message (str): User's input message.\n",
        "        history (list): Past conversation history.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response from the Llama model.\n",
        "    \"\"\"\n",
        "    query = format_message(message, history)\n",
        "    response = \"\"\n",
        "\n",
        "    sequences = llama_pipeline(\n",
        "        query,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=1024,\n",
        "    )\n",
        "\n",
        "    generated_text = sequences[0]['generated_text']\n",
        "    response = generated_text[len(query):]  # Remove the prompt from the output\n",
        "\n",
        "    print(\"Chatbot:\", response.strip())\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "4Xu4ya5pTlLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and Launching the Chat Interface with Gradio\n",
        "\n",
        "With the LLaMa model set up and ready to generate responses, the final step is to create a user interface for interacting with the chatbot.\n",
        "\n",
        "In this step, we'll use Gradio to create a chat interface for our chatbot and launch it.\n"
      ],
      "metadata": {
        "id": "wFBoYXWipL-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(get_llama_response).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "fH27XMiGTrXv",
        "outputId": "c7ccf0e2-0e2d-4643-be36-fcd8b8440425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1b205e9dbfc805d2d8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1b205e9dbfc805d2d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We have successfully built an interactive chatbot leveraging the LLaMa 2 model for natural language processing and Gradio for user interface creation.\n",
        "\n",
        "With the skills acquired, you are now equipped to further explore and experiment with different large language models, refine the chatbot's performance, or even deploy it in real-world applications. Keep experimenting, and continue expanding your knowledge in the realm of conversational AI and user interface design!\n"
      ],
      "metadata": {
        "id": "_oyHsLGHpgLK"
      }
    }
  ]
}